{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7953460c-33d0-4fb1-a257-35740a9f75ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 10\n",
      "Epoch 1/100, Train Loss: 0.7735, Val Loss: 0.6535, Val Acc: 76.37%\n",
      "Epoch 2/100, Train Loss: 0.6247, Val Loss: 0.6110, Val Acc: 76.97%\n",
      "Epoch 3/100, Train Loss: 0.5878, Val Loss: 0.5780, Val Acc: 78.27%\n",
      "Epoch 4/100, Train Loss: 0.5645, Val Loss: 0.5590, Val Acc: 78.71%\n",
      "Epoch 5/100, Train Loss: 0.5503, Val Loss: 0.5465, Val Acc: 79.18%\n",
      "Epoch 6/100, Train Loss: 0.5391, Val Loss: 0.5324, Val Acc: 79.63%\n",
      "Epoch 7/100, Train Loss: 0.5312, Val Loss: 0.5300, Val Acc: 79.64%\n",
      "Epoch 8/100, Train Loss: 0.5249, Val Loss: 0.5261, Val Acc: 79.97%\n",
      "Epoch 9/100, Train Loss: 0.5189, Val Loss: 0.5153, Val Acc: 79.69%\n",
      "Epoch 10/100, Train Loss: 0.5137, Val Loss: 0.5183, Val Acc: 79.71%\n",
      "Epoch 11/100, Train Loss: 0.5092, Val Loss: 0.5130, Val Acc: 80.12%\n",
      "Epoch 12/100, Train Loss: 0.5050, Val Loss: 0.5095, Val Acc: 79.92%\n",
      "Epoch 13/100, Train Loss: 0.5009, Val Loss: 0.5020, Val Acc: 80.36%\n",
      "Epoch 14/100, Train Loss: 0.4972, Val Loss: 0.5071, Val Acc: 79.97%\n",
      "Epoch 15/100, Train Loss: 0.4944, Val Loss: 0.4978, Val Acc: 80.40%\n",
      "Epoch 16/100, Train Loss: 0.4916, Val Loss: 0.4949, Val Acc: 80.67%\n",
      "Epoch 17/100, Train Loss: 0.4887, Val Loss: 0.4923, Val Acc: 80.56%\n",
      "Epoch 18/100, Train Loss: 0.4858, Val Loss: 0.4900, Val Acc: 80.80%\n",
      "Epoch 19/100, Train Loss: 0.4838, Val Loss: 0.4867, Val Acc: 81.05%\n",
      "Epoch 20/100, Train Loss: 0.4815, Val Loss: 0.4880, Val Acc: 80.54%\n",
      "Epoch 21/100, Train Loss: 0.4798, Val Loss: 0.4855, Val Acc: 81.10%\n",
      "Epoch 22/100, Train Loss: 0.4779, Val Loss: 0.4850, Val Acc: 80.56%\n",
      "Epoch 23/100, Train Loss: 0.4763, Val Loss: 0.4883, Val Acc: 80.79%\n",
      "Epoch 24/100, Train Loss: 0.4750, Val Loss: 0.4831, Val Acc: 81.07%\n",
      "Epoch 25/100, Train Loss: 0.4729, Val Loss: 0.4808, Val Acc: 81.25%\n",
      "Epoch 26/100, Train Loss: 0.4720, Val Loss: 0.4802, Val Acc: 80.95%\n",
      "Epoch 27/100, Train Loss: 0.4705, Val Loss: 0.4771, Val Acc: 81.18%\n",
      "Epoch 28/100, Train Loss: 0.4690, Val Loss: 0.4838, Val Acc: 80.66%\n",
      "Epoch 29/100, Train Loss: 0.4681, Val Loss: 0.4748, Val Acc: 81.45%\n",
      "Epoch 30/100, Train Loss: 0.4664, Val Loss: 0.4764, Val Acc: 81.16%\n",
      "Epoch 31/100, Train Loss: 0.4655, Val Loss: 0.4724, Val Acc: 81.51%\n",
      "Epoch 32/100, Train Loss: 0.4642, Val Loss: 0.4723, Val Acc: 81.43%\n",
      "Epoch 33/100, Train Loss: 0.4630, Val Loss: 0.4745, Val Acc: 81.26%\n",
      "Epoch 34/100, Train Loss: 0.4617, Val Loss: 0.4713, Val Acc: 81.27%\n",
      "Epoch 35/100, Train Loss: 0.4611, Val Loss: 0.4726, Val Acc: 81.24%\n",
      "Epoch 36/100, Train Loss: 0.4604, Val Loss: 0.4694, Val Acc: 81.39%\n",
      "Epoch 37/100, Train Loss: 0.4588, Val Loss: 0.4745, Val Acc: 81.16%\n",
      "Epoch 38/100, Train Loss: 0.4581, Val Loss: 0.4688, Val Acc: 81.60%\n",
      "Epoch 39/100, Train Loss: 0.4567, Val Loss: 0.4696, Val Acc: 81.52%\n",
      "Epoch 40/100, Train Loss: 0.4562, Val Loss: 0.4685, Val Acc: 81.37%\n",
      "Epoch 41/100, Train Loss: 0.4553, Val Loss: 0.4716, Val Acc: 81.31%\n",
      "Epoch 42/100, Train Loss: 0.4543, Val Loss: 0.4702, Val Acc: 81.51%\n",
      "Epoch 43/100, Train Loss: 0.4538, Val Loss: 0.4677, Val Acc: 81.53%\n",
      "Epoch 44/100, Train Loss: 0.4530, Val Loss: 0.4650, Val Acc: 81.78%\n",
      "Epoch 45/100, Train Loss: 0.4522, Val Loss: 0.4655, Val Acc: 81.51%\n",
      "Epoch 46/100, Train Loss: 0.4513, Val Loss: 0.4663, Val Acc: 81.72%\n",
      "Epoch 47/100, Train Loss: 0.4507, Val Loss: 0.4669, Val Acc: 81.41%\n",
      "Epoch 48/100, Train Loss: 0.4502, Val Loss: 0.4658, Val Acc: 81.61%\n",
      "Epoch 49/100, Train Loss: 0.4491, Val Loss: 0.4651, Val Acc: 81.51%\n",
      "Epoch 50/100, Train Loss: 0.4488, Val Loss: 0.4636, Val Acc: 81.77%\n",
      "Epoch 51/100, Train Loss: 0.4480, Val Loss: 0.4656, Val Acc: 81.31%\n",
      "Epoch 52/100, Train Loss: 0.4473, Val Loss: 0.4629, Val Acc: 81.86%\n",
      "Epoch 53/100, Train Loss: 0.4466, Val Loss: 0.4627, Val Acc: 81.57%\n",
      "Epoch 54/100, Train Loss: 0.4461, Val Loss: 0.4660, Val Acc: 81.50%\n",
      "Epoch 55/100, Train Loss: 0.4455, Val Loss: 0.4625, Val Acc: 81.71%\n",
      "Epoch 56/100, Train Loss: 0.4450, Val Loss: 0.4619, Val Acc: 81.83%\n",
      "Epoch 57/100, Train Loss: 0.4442, Val Loss: 0.4613, Val Acc: 81.78%\n",
      "Epoch 58/100, Train Loss: 0.4438, Val Loss: 0.4637, Val Acc: 81.48%\n",
      "Epoch 59/100, Train Loss: 0.4433, Val Loss: 0.4602, Val Acc: 81.66%\n",
      "Epoch 60/100, Train Loss: 0.4427, Val Loss: 0.4608, Val Acc: 81.90%\n",
      "Epoch 61/100, Train Loss: 0.4426, Val Loss: 0.4595, Val Acc: 81.83%\n",
      "Epoch 62/100, Train Loss: 0.4419, Val Loss: 0.4603, Val Acc: 81.94%\n",
      "Epoch 63/100, Train Loss: 0.4414, Val Loss: 0.4594, Val Acc: 81.77%\n",
      "Epoch 64/100, Train Loss: 0.4410, Val Loss: 0.4590, Val Acc: 81.88%\n",
      "Epoch 65/100, Train Loss: 0.4404, Val Loss: 0.4593, Val Acc: 81.76%\n",
      "Epoch 66/100, Train Loss: 0.4399, Val Loss: 0.4599, Val Acc: 81.86%\n",
      "Epoch 67/100, Train Loss: 0.4397, Val Loss: 0.4598, Val Acc: 81.86%\n",
      "Epoch 68/100, Train Loss: 0.4393, Val Loss: 0.4584, Val Acc: 81.85%\n",
      "Epoch 69/100, Train Loss: 0.4390, Val Loss: 0.4597, Val Acc: 81.65%\n",
      "Epoch 70/100, Train Loss: 0.4385, Val Loss: 0.4600, Val Acc: 81.75%\n",
      "Epoch 71/100, Train Loss: 0.4381, Val Loss: 0.4594, Val Acc: 81.79%\n",
      "Epoch 72/100, Train Loss: 0.4378, Val Loss: 0.4584, Val Acc: 81.86%\n",
      "Epoch 73/100, Train Loss: 0.4375, Val Loss: 0.4587, Val Acc: 81.93%\n",
      "Epoch 74/100, Train Loss: 0.4373, Val Loss: 0.4576, Val Acc: 81.96%\n",
      "Epoch 75/100, Train Loss: 0.4369, Val Loss: 0.4572, Val Acc: 81.96%\n",
      "Epoch 76/100, Train Loss: 0.4365, Val Loss: 0.4572, Val Acc: 81.92%\n",
      "Epoch 77/100, Train Loss: 0.4363, Val Loss: 0.4582, Val Acc: 81.89%\n",
      "Epoch 78/100, Train Loss: 0.4361, Val Loss: 0.4573, Val Acc: 81.93%\n",
      "Epoch 79/100, Train Loss: 0.4356, Val Loss: 0.4574, Val Acc: 81.92%\n",
      "Epoch 80/100, Train Loss: 0.4355, Val Loss: 0.4574, Val Acc: 81.82%\n",
      "Epoch 81/100, Train Loss: 0.4353, Val Loss: 0.4575, Val Acc: 81.85%\n",
      "Epoch 82/100, Train Loss: 0.4352, Val Loss: 0.4566, Val Acc: 82.00%\n",
      "Epoch 83/100, Train Loss: 0.4349, Val Loss: 0.4572, Val Acc: 82.02%\n",
      "Epoch 84/100, Train Loss: 0.4347, Val Loss: 0.4567, Val Acc: 82.01%\n",
      "Epoch 85/100, Train Loss: 0.4345, Val Loss: 0.4564, Val Acc: 82.06%\n",
      "Epoch 86/100, Train Loss: 0.4344, Val Loss: 0.4564, Val Acc: 81.94%\n",
      "Epoch 87/100, Train Loss: 0.4342, Val Loss: 0.4565, Val Acc: 81.98%\n",
      "Epoch 88/100, Train Loss: 0.4340, Val Loss: 0.4564, Val Acc: 82.03%\n",
      "Epoch 89/100, Train Loss: 0.4339, Val Loss: 0.4567, Val Acc: 81.89%\n",
      "Epoch 90/100, Train Loss: 0.4338, Val Loss: 0.4563, Val Acc: 82.03%\n",
      "Epoch 91/100, Train Loss: 0.4337, Val Loss: 0.4563, Val Acc: 82.07%\n",
      "Epoch 92/100, Train Loss: 0.4336, Val Loss: 0.4563, Val Acc: 82.01%\n",
      "Epoch 93/100, Train Loss: 0.4335, Val Loss: 0.4564, Val Acc: 82.06%\n",
      "Epoch 94/100, Train Loss: 0.4334, Val Loss: 0.4563, Val Acc: 82.04%\n",
      "Epoch 95/100, Train Loss: 0.4333, Val Loss: 0.4561, Val Acc: 82.09%\n",
      "Epoch 96/100, Train Loss: 0.4333, Val Loss: 0.4562, Val Acc: 82.04%\n",
      "Epoch 97/100, Train Loss: 0.4332, Val Loss: 0.4562, Val Acc: 82.05%\n",
      "Epoch 98/100, Train Loss: 0.4332, Val Loss: 0.4562, Val Acc: 82.05%\n",
      "Epoch 99/100, Train Loss: 0.4332, Val Loss: 0.4562, Val Acc: 82.05%\n",
      "Epoch 100/100, Train Loss: 0.4331, Val Loss: 0.4562, Val Acc: 82.05%\n",
      "Model saved to ./model.pth\n",
      "Attack category counts in test data:\n",
      "Normal: 56000 samples\n",
      "Fuzzers: 18184 samples\n",
      "Analysis: 2000 samples\n",
      "Backdoor: 1746 samples\n",
      "DoS: 12264 samples\n",
      "Exploits: 33393 samples\n",
      "Generic: 40000 samples\n",
      "Reconnaissance: 10491 samples\n",
      "Shellcode: 1133 samples\n",
      "Worms: 130 samples\n",
      "모델 예측\n",
      "Category counts:\n",
      "Normal: 55108 samples\n",
      "Fuzzers: 17691 samples\n",
      "Analysis: 372 samples\n",
      "Backdoor: 182 samples\n",
      "DoS: 4155 samples\n",
      "Exploits: 48208 samples\n",
      "Generic: 39253 samples\n",
      "Reconnaissance: 9236 samples\n",
      "Shellcode: 1110 samples\n",
      "Worms: 26 samples\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 라이브러리 임포트\n",
    "import torch  # PyTorch 텐서 및 신경망 연산을 위한 핵심 라이브러리\n",
    "import torch.nn as nn  # 신경망 모델을 정의할 때 필요한 클래스와 함수들\n",
    "import torch.optim as optim  # 옵티마이저를 위한 라이브러리 (예: Adam, SGD)\n",
    "import torch.nn.functional as F  # 신경망에서 자주 사용하는 함수들 (예: ReLU, CrossEntropyLoss)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터셋 처리 및 DataLoader 제공\n",
    "\n",
    "# 데이터 처리 및 분석을 위한 라이브러리\n",
    "import pandas as pd  # 데이터프레임을 다루는 데 사용되는 라이브러리 (CSV 파일 읽기 등)\n",
    "import numpy as np  # 고성능 수치 연산을 위한 라이브러리 (배열 및 행렬 연산)\n",
    "\n",
    "# 데이터 전처리를 위한 scikit-learn의 도구들\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # 범주형 데이터를 숫자로 변환 및 데이터 스케일링\n",
    "\n",
    "# 모델과 전처리된 데이터를 파일로 저장하고 불러오기 위한 라이브러리\n",
    "import joblib  # 객체를 파일로 직렬화하여 저장하거나 불러오는 데 사용\n",
    "\n",
    "\n",
    "\n",
    "# CNN 모델 정의\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # 1D Convolutional Layer 정의\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        # Max Pooling Layer 정의\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        # Fully Connected Layer 정의\n",
    "        self.fc1 = nn.Linear(64 * (input_size // 2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv1D 입력을 위한 채널 추가\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = F.relu(self.conv1(x))  # 첫 번째 Convolutional Layer\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 두 번째 Convolutional Layer + Max Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))  # Fully Connected Layer 1\n",
    "        x = self.fc2(x)  # 출력 레이어\n",
    "        return x\n",
    "\n",
    "# 데이터 준비 함수\n",
    "def prepare_training_data(data_path, is_training=True):\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # 데이터에서 'id' 컬럼을 삭제\n",
    "    if 'id' in data.columns:\n",
    "        data = data.drop('id', axis=1)\n",
    "    \n",
    "    # 공격 카테고리 정의 및 매핑\n",
    "    attack_categories = ['Normal', 'Fuzzers', 'Analysis', 'Backdoor', 'DoS', 'Exploits', \n",
    "                         'Generic', 'Reconnaissance', 'Shellcode', 'Worms']\n",
    "    attack_cat_map = {cat: idx for idx, cat in enumerate(attack_categories)}\n",
    "    data['attack_cat'] = data['attack_cat'].map(attack_cat_map)\n",
    "\n",
    "    # Null 값이 있는 행을 삭제\n",
    "    if data['attack_cat'].isnull().any():\n",
    "        data = data.dropna(subset=['attack_cat'])\n",
    "\n",
    "    # 훈련일 경우, 'attack_cat'과 'label'을 제외한 데이터를 X로 설정하고 'attack_cat'을 y로 설정\n",
    "    if is_training:\n",
    "        y = data['attack_cat'].values\n",
    "        X = data.drop(['attack_cat', 'label'], axis=1)\n",
    "    else:\n",
    "        X = data.drop(['attack_cat', 'label'], axis=1)\n",
    "\n",
    "    # 범주형 컬럼에 대해 라벨 인코딩\n",
    "    categorical_cols = ['proto', 'service', 'state', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd']\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        X[col] = label_encoders[col].fit_transform(X[col].astype(str))\n",
    "\n",
    "    # 데이터 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # numpy float32로 형 변환\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    # 인코더와 스케일러 저장\n",
    "    joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    \n",
    "    return X, y, label_encoders, scaler\n",
    "\n",
    "def prepare_test_data(data_path):\n",
    "    # 데이터 로드: CSV 파일에서 데이터 읽기\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # 'id' 컬럼이 있으면 제거\n",
    "    if 'id' in data.columns:\n",
    "        data = data.drop('id', axis=1)\n",
    "    \n",
    "    # 공격 카테고리 목록 정의\n",
    "    attack_categories = ['Normal', 'Fuzzers', 'Analysis', 'Backdoor', 'DoS', 'Exploits', \n",
    "                         'Generic', 'Reconnaissance', 'Shellcode', 'Worms']\n",
    "    # 공격 카테고리별 인덱스 매핑\n",
    "    attack_cat_map = {cat: idx for idx, cat in enumerate(attack_categories)}\n",
    "\n",
    "    # 'attack_cat' 컬럼의 문자열 값을 정수로 변환\n",
    "    data['attack_cat'] = data['attack_cat'].map(attack_cat_map)\n",
    "\n",
    "    # 'attack_cat' 값이 NaN인 행 제거\n",
    "    if data['attack_cat'].isnull().any():\n",
    "        data = data.dropna(subset=['attack_cat'])\n",
    "\n",
    "    # 'attack_cat'과 'label' 컬럼을 제외한 모든 컬럼을 X로 설정\n",
    "    X = data.drop(['attack_cat', 'label'], axis=1)\n",
    "\n",
    "    # 학습된 레이블 인코더와 스케일러 불러오기\n",
    "    label_encoders = joblib.load('label_encoders.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    # 범주형 컬럼을 정수로 변환\n",
    "    categorical_cols = ['proto', 'service', 'state', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd']\n",
    "    for col in categorical_cols:\n",
    "        # 각 범주형 컬럼에 대해 인코딩 적용\n",
    "        X[col] = label_encoders[col].transform(X[col].astype(str))\n",
    "\n",
    "    # 스케일러를 사용하여 특성 값 정규화\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # X를 float32 타입으로 변환 (모델에 맞는 데이터 타입)\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    return X\n",
    "    \n",
    "# 훈련 함수\n",
    "def train(device, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # 미니배치로 훈련\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)  # 데이터와 레이블을 device로 이동\n",
    "        optimizer.zero_grad()  # 이전 그래디언트 초기화\n",
    "        output = model(x)  # 모델을 통해 예측\n",
    "        loss = criterion(output, y)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 옵티마이저 단계 진행\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)  # 평균 손실 반환\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(device, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    # 평가 모드로 변환 후, 기울기 계산 안함\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)  # 데이터와 레이블을 device로 이동\n",
    "            output = model(x)  # 모델을 통해 예측\n",
    "            loss = criterion(output, y)  # 손실 계산\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)  # 예측값이 가장 큰 인덱스 선택\n",
    "            correct += pred.eq(y).sum().item()  # 정확도 계산\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy  # 평균 손실과 정확도 반환\n",
    "\n",
    "\n",
    "# 테스트 함수\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch[0].to(device)  # 배치 데이터를 device로 이동\n",
    "            outputs = model(X_batch)  # 모델 예측\n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 큰 값의 인덱스를 예측값으로 사용\n",
    "            predictions.extend(predicted.cpu().numpy())  # 예측값 저장\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 공격 카테고리 개수 출력 함수\n",
    "def check_attack_category_counts(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # 공격 카테고리 목록\n",
    "    attack_categories = ['Normal', 'Fuzzers', 'Analysis', 'Backdoor', 'DoS', 'Exploits', \n",
    "                         'Generic', 'Reconnaissance', 'Shellcode', 'Worms']\n",
    "    \n",
    "    # 각 공격 카테고리의 샘플 개수 출력\n",
    "    attack_counts = data['attack_cat'].value_counts()\n",
    "    print(\"Attack category counts in test data:\")\n",
    "    for category in attack_categories:\n",
    "        count = attack_counts.get(category, 0)\n",
    "        print(f\"{category}: {count} samples\")\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 또는 CPU 설정\n",
    "    filepath = './training-set.csv'\n",
    "    \n",
    "    # 훈련 데이터 준비\n",
    "    X_train, y_train, label_encoders, scaler = prepare_training_data('./training-set.csv', is_training=True)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 데이터셋 및 DataLoader 생성\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 설정\n",
    "    model = CNN(input_size=X_train.shape[1], num_classes=len(np.unique(y_train))).to(device)\n",
    "    print(\"num_classes:\", len(np.unique(y_train)))\n",
    "    criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss는 다중 클래스 분류에서 자주 사용됨\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adam 옵티마이저 사용\n",
    "    \n",
    "    # 학습 루프 설정\n",
    "    epochs = 100\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # 학습률 스케줄러 설정\n",
    "    for epoch in range(epochs):\n",
    "        # 훈련 및 검증\n",
    "        train_loss = train(device, model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = validate(device, model, val_loader, criterion)\n",
    "        scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "        # 에폭, 훈련 손실, 검증 손실, 검증 정확도 출력\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    # 모델 저장\n",
    "    model_path = './model.pth'\n",
    "    torch.save(model.state_dict(), model_path)  # 모델 상태 저장\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    X_test = prepare_test_data(filepath)\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 테스트\n",
    "    predictions = test(model, test_loader, device)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"Test predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ae7ce-7e41-4470-b0e7-6b5577a3779f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ev",
   "language": "python",
   "name": "torch_ev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
